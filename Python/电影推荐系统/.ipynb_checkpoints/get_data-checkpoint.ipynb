{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "爬取信息保存完毕!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#下面是爬虫所使用的库\n",
    "import re   #正则表达式\n",
    "import urllib.request,urllib.error   #指定url,获取网页数据\n",
    "import xlwt     #进行Excel操作\n",
    "import csv      #进行csv操作\n",
    "from bs4 import BeautifulSoup   #网页解析\n",
    "\n",
    "#注意下面路径中的必须是“start=”，因为我们通过手动添加start的值进行锁定爬取页面\n",
    "src=\"https://www.mi.com/\"\n",
    "\n",
    "#获取网页源码\n",
    "def gethtml(url):\n",
    "    head={\n",
    "            \"User-Agent\": \"Mozilla / 5.0(Windows NT 10.0;Win64;x64) AppleWebKit / 537.36(KHTML, likeGecko) Chrome / 83.0.4103.116Safari / 537.36\"\n",
    "        }\n",
    "    #User-Agent的信息为Chrome的信息，请将默认浏览器设置为Chrome浏览器\n",
    "    req=urllib.request.Request(url,headers=head)\n",
    "    response=urllib.request.urlopen(req)\n",
    "    html=response.read().decode(\"utf-8\",\"ignore\")\n",
    "    # print(html)\n",
    "    soup=BeautifulSoup(html,\"html.parser\")\n",
    "    # print(soup)\n",
    "    return soup\n",
    "soups=gethtml(src)\n",
    "\n",
    "#爬取信息的正则表达式\n",
    "findtupianlink=re.compile(r'data-src=\"(.*?)\" height=\"110\" src=\"//i1.mifile.cn/f/i/2014/cn/placeholder-220!110x110.png\"',re.S)\n",
    "findshangpinlink=re.compile(r'<a data-log_code=\"(.*?)\" href=\"(.*?)\" target=\"_blank\">',re.S)\n",
    "findname=re.compile(r'<div class=\"title\">(.*?)</div>',re.S)\n",
    "findmoney=re.compile(r'<p class=\"price\">(.*?)(元|元起)</p>',re.S)\n",
    "\n",
    "datalist=[]\n",
    "# for item in soups.find_all('li',class_=\"\"):\n",
    "for item in soups.find_all('a'):\n",
    "    # print(item)\n",
    "    item=str(item)\n",
    "    data=[]\n",
    "    tupian = re.findall(findtupianlink, item)\n",
    "    if len(tupian)< 1:\n",
    "        continue\n",
    "    else:\n",
    "        data.append(tupian[0])\n",
    "    shangpin = re.findall(findshangpinlink, item)\n",
    "    if len(shangpin) < 1:\n",
    "        continue\n",
    "    else:  #因为a data-log_code=\"(.*?)\" href=\"(.*?)\" target=\"_blank\">，所以会返回一个元组(\"\",\"\")\n",
    "           #且保存在shangpin[0]中\n",
    "           #我们想获得元组的的第二个元素，所以要shangpin[0][1]\n",
    "        data.append(shangpin[0][1])\n",
    "    name = re.findall(findname, item)\n",
    "    if len(name) < 1:\n",
    "        continue\n",
    "    else:\n",
    "        data.append(name[0])\n",
    "    money = re.findall(findmoney, item)\n",
    "    if len(money) < 1:\n",
    "        continue\n",
    "    else:\n",
    "        data.append(money[0][0])\n",
    "    datalist.append(data)\n",
    "for i in datalist:\n",
    "    for j in i:\n",
    "        print(j,end=\"\\t\")\n",
    "    print()\n",
    "print(len(datalist))\n",
    "\n",
    "#将list中的信息写入txt文档\n",
    "f=open(\"xiaomi.txt\",\"w\",encoding=\"utf-8\")\n",
    "for i in datalist:\n",
    "     # print(\"已写入第%d条\" % (i + 1))\n",
    "     for j in i:\n",
    "         string=str(j) #转换成字符串\n",
    "         f.write(string) #写入\n",
    "         f.write(\",\") #用','进行隔开\n",
    "     f.write(\"\\n\")\n",
    "f.close\n",
    "\n",
    "#将list中的信息存到Excel中\n",
    "book = xlwt.Workbook(encoding=\"utf-8\",style_compression=0)  # 创建book对象\n",
    "sheet = book.add_sheet('mi',cell_overwrite_ok=True)  # 创建工作表\n",
    "col=(\"商品图片连接\",\"商品链接\",\"商品名\",\"价格\")\n",
    "for i in range(len(col)):\n",
    "    sheet.write(0,i,col[i])  #将col中的信息写入第一行的1-4列\n",
    "for i in range(0,len(datalist)):\n",
    "    # print(\"已写入第%d条\"%(i+1))\n",
    "    data=datalist[i]\n",
    "    for j in range(0,4):\n",
    "        sheet.write(i+1,j,data[j])\n",
    "book.save(\"xiaomi.xls\")\n",
    "\n",
    "#将list中的数据存储到csv文件中\n",
    "with open('xiaomi.csv', 'w', encoding='utf-8',newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=',') # 这里在初始化写入对象时传入delimiter为','，此时输出结果的每一列就是以','分隔了\n",
    "    writer.writerow([\"商品图片连接\",\"商品链接\",\"商品名\",\"价格\"])\n",
    "    for i in datalist: #将datalist中的列表循环写入\n",
    "        writer.writerow(i)\n",
    "\n",
    "print(\"爬取信息保存完毕!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
